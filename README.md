--- 
<div align="center">    
  
# `SSL`: Korean Disaster `S`afety Information<br>`S`ign `L`anguage Translation Benchmark Dataset   

[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](http://smartweb.yonsei.ac.kr/)
[![Conference](http://img.shields.io/badge/COLING-2024-4b44ce.svg)](https://lrec-coling-2024.org/)
[![Conference](https://img.shields.io/badge/LREC-2024-4b44ce.svg)](https://lrec-coling-2024.org/)

This repo contains data preprocessing code for the paper : [Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset]()
</div>

# Requirements  
1. Go to **AIhub** to download the original raw data [Link](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=636).
2. **Unzip** the data you received in 1.
3. **Clone** the repository:
```bash
git clone https://github.com/SSL-Sign-Language/Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset.git
```
4. Navigate to the project directory and install the required dependencies:
```bash
cd ./Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset
[Optional] conda create -n ssl_sign_data python=3.9
pip install -r requirements.txt
```
# Folder and File Structure
- `README.md`: Contains an overview and description of the project
- `main.py`: The main execution file for the project
- `requirements.txt`: Lists the dependencies required for the project
- `src/`: Contains the source code for the project
  - `__init__.py`: Package initialization file
  - `args.py`: Handles command-line arguments
  - `keypoint_extractor.py`: Module for extracting keypoints
  - `language_processor.py`: Module for processing Language & Gloss
  - `processor.py`: General processing module
  - `sign_processor.py`: Module for processing the full preprocessing suggested in the paper
  - `video_processor.py`: Module for processing videos
- `visualize_keypoint.ipynb`: Jupyter notebook for keypoint visualization

# How to run   
Once you've completed your configuration, it's very easy to run data preprocessing.

```bash
python main.py --root_path <path_to_downloaded_data> --save_path <path_to_save_results>
```
> [!NOTE]
>
> For other arguments, see `args.py`. Since the default value is set, you only need to set `--root_path` to run the code.
> 
> If you don't set `--save_path`, it will automatically create `./result` as a result folder.
> 
> If you want to visualize keypoints on images, check the `visualize_keypoint.ipynb`.


# Result Structure
After running `main.py`, the following folder structure will be generated:
- `result/`: Contains the output generated by `main.py`.
  - `Train/`: Contains training data results.
    - `Keypoint/`: npy files is saved with the extracted keypoints for each frame of the Sign Video.
    - `Language/`: json and vocab files are saved.
    - `Video/`: Preprocesses the video frame by frame, saving each frame.
  - `Validation/` : Same as the Train structure.
    - `Keypoint/`
    - `Language/`
    - `Video/`

# Citation
Please cite the paper below if you use this code in your research:

**Coming soon**
```
@article{YourName,
  title={Your Title},
  author={Your team},
  journal={Location},
  year={Year}
}
```
If you have any questions, please contact us [here](http://smartweb.yonsei.ac.kr/).

# Acknowledgements
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C100697011).

This work is financially supported by Korea Ministry of Land, Infrastructure and Transport(MOLIT) as Innovative Talent Education Program for Smart City.

Thank you to Eunyoung Lee and Donghyun Kim from the Institute of Korean Sign Language for their valuable insight to the research.
