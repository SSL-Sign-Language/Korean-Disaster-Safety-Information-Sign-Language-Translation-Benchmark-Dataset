--- 
<div align="center">    
  
# `SSL`: Korean Disaster `S`afety Information<br>`S`ign `L`anguage Translation Benchmark Dataset   

[![Conference](http://img.shields.io/badge/COLING-2024-4b44ce.svg)](https://lrec-coling-2024.org/)
[![Conference](https://img.shields.io/badge/LREC-2024-4b44ce.svg)](https://lrec-coling-2024.org/)

[Korean Disaster Safety Information Sign Language Translation Benchmark Dataset](https://aclanthology.org/2024.lrec-main.869) (Kim et al., LREC-COLING 2024)

</div>


# Requirements  
1. Go to **AIhub** to download the original raw data [Link](https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=636).
2. **Unzip** the data you received in 1.
3. **Clone** the repository:
```bash
git clone https://github.com/SSL-Sign-Language/Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset.git
```
4. Navigate to the project directory and install the required dependencies:
```bash
cd ./Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset
[Optional] conda create -n ssl_sign_data python=3.9
pip install -r requirements.txt
```
# Folder and File Structure
- `README.md`: Contains an overview and description of the project
- `main.py`: The main execution file for the project
- `requirements.txt`: Lists the dependencies required for the project
- `src/`: Contains the source code for the project
  - `__init__.py`: Package initialization file
  - `args.py`: Handles command-line arguments
  - `keypoint_extractor.py`: Module for extracting keypoints
  - `language_processor.py`: Module for processing Language & Gloss
  - `processor.py`: General processing module
  - `sign_processor.py`: Module for processing the full preprocessing suggested in the paper
  - `video_processor.py`: Module for processing videos
- `visualize_keypoint.ipynb`: Jupyter notebook for keypoint visualization

# How to run   
Once you've completed your configuration, it's very easy to run data preprocessing.

```bash
python main.py --root_path <path_to_downloaded_data> --save_path <path_to_save_results>
```
> [!NOTE]
>
> For other arguments, see `args.py`. Since the default value is set, you only need to set `--root_path` to run the code.
> 
> If you don't set `--save_path`, it will automatically create `./result` as a result folder.
> 
> If you want to visualize keypoints on images, check the `visualize_keypoint.ipynb`.


# Result Structure
After running `main.py`, the following folder structure will be generated:
- `result/`: Contains the output generated by `main.py`.
  - `Train/`: Contains training data results.
    - `Keypoint/`: npy files is saved with the extracted keypoints for each frame of the Sign Video.
    - `Language/`: json and vocab files are saved.
    - `Video/`: Preprocesses the video frame by frame, saving each frame.
  - `Validation/` : Same as the Train structure.
    - `Keypoint/`
    - `Language/`
    - `Video/`

# Citation
Please cite the paper below if you use this code in your research:
```
@inproceedings{kim-etal-2024-korean-disaster,
    title = "{K}orean Disaster Safety Information Sign Language Translation Benchmark Dataset",
    author = "Kim, Wooyoung  and
      Kim, TaeYong  and
      Kim, Byeongjin  and
      Lee, Myeong Jin MJ  and
      Lee, Gitaek  and
      Kim, Kirok  and
      Cha, Jisoo  and
      Kim, Wooju",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italy",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.869",
    pages = "9948--9953",
    abstract = "Sign language is a crucial means of communication for deaf communities. However, those outside deaf communities often lack understanding of sign language, leading to inadequate communication accessibility for the deaf. Therefore, sign language translation is a significantly important research area. In this context, we present a new benchmark dataset for Korean sign language translation named SSL:korean disaster Safety information Sign Language translation benchmark dataset. Korean sign language translation datasets provided by the National Information Society Agency in South Korea have faced challenges related to computational resources, heterogeneity between train and test sets, and unrefined data. To alleviate the aforementioned issue, we refine the origin data and release them. Additionally, we report experimental results of baseline using a transformer architecture. We empirically demonstrate that the baseline performance varies depending on the tokenization method applied to gloss sequences. In particular, tokenization based on characteristics of sign language outperforms tokenization considering characteristics of spoken language and tokenization utilizing statistical techniques. We release materials at our https://github.com/SSL-Sign-Language/Korean-Disaster-Safety-Information-Sign-Language-Translation-Benchmark-Dataset",
}
```
If you have any questions, please contact us [here](http://smartweb.yonsei.ac.kr/).

# Acknowledgements
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. 2023R1A2C100697011).

This work is financially supported by Korea Ministry of Land, Infrastructure and Transport(MOLIT) as Innovative Talent Education Program for Smart City.

Thank you to Eunyoung Lee and Donghyun Kim from the Institute of Korean Sign Language for their valuable insight to the research.
